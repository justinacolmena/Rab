\documentclass[letterpaper,12pt]{article}         % the type of document and font size (default 10pt)
\usepackage{lmodern}
\usepackage[T1,T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[margin=1.0in]{geometry}   % sets all margins to 1in, can be changed
\usepackage{moreverb}                 % for verbatimtabinput -- LaTeX environment
\usepackage{url}                      % for \url{} command
\usepackage{amssymb}                  % for many mathematical symbols
\usepackage[pdftex]{lscape}           % for landscaped tables
\usepackage{longtable}                % for tables that break over multiple pages
\usepackage{hyperref}
\hypersetup{
	colorlinks=true, %set true if you want colored links
	linktoc=all,     %set to all if you want both sections and subsections linked
	linkcolor=blue,  %choose some color if you want links to stand out
}
\usepackage{color}

\usepackage[numbers]{natbib}
\bibliographystyle{plainnat}

\title{\textsf{R}: $\alpha$, and $\beta$}  % to specify title
\author{justina colmena}          % to specify author(s)
\begin{document}                      % document begins here

% If .nw file contains graphs: To specify that EPS/PDF graph files are to be 
% saved to 'graphics' sub-folder
%     NOTE: 'graphics' sub-folder must exist prior to Sweave step
%\SweaveOpts{prefix.string=graphics/plot}

% If .nw file contains graphs: to modify (shrink/enlarge} size of graphics 
% file inserted
%         NOTE: can be specified/modified before any graph chunk
\setkeys{Gin}{width=1.0\textwidth}

\maketitle              % makes the title
\tableofcontents        % inserts TOC (section, sub-section, etc numbers and titles)
\listoftables           % inserts LOT (numbers and captions)
\listoffigures          % inserts LOF (numbers and captions)
%                        %     NOTE: graph chunk must be wrapped with \begin{figure}, 
%                        %  \end{figure}, and \caption{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Where everything else goes

\section{Prerequisites}
Before starting this project, we were somewhat familiar with \textsf{R} \cite{r-project} and \LaTeX \cite{latex-project}, as well as with Donald Knuth's philosophy of ``literate programming'' \cite{knuth1992}, but we had not yet used the ``Sweave'' function \cite{geyer-Sweave} of \textsf{R}, whose official documentation seems to have disappeared along with its author \cite{leisch2018}.  Fortunately for our purposes, we found an easy Sweave template \cite{kerns2011} online, which we adapted into this paper.

\section{Getting started}
We are going to calculate the beta of a stock using free data \cite{fred,quandl} in \textsf{R} \cite{r-project}.
Nowadays it is fashionable to use APIs \cite{boysel-fredr,cran-quandl} to fetch the data for analysis, which is very
convenient once it is set up, even though it is somewhat technical and a little bit of work to get it working.

Once the API packages are installed, we must load them from the library in order to be able to use them in our
\textsf{R} session. We also need to load our API account keys for access credentials to use the data.

<<>>=
library('dplyr')
library('xts')
library('fredr')
library('Quandl')
source('my_api_keys.R')

fredr_key(my_fred_api_key)
Quandl.api_key(my_quandl_api_key)
@

\section{Fetching the data}
To calculate the beta of a stock, we need a major market index, an index of returns on a
so-called ``risk-free interest rate'' and returns for our stock of interest. Let us fetch some data
for the S\&P 500 Index, the Federal Runds rate, and our stock of interest \cite{coeur}.

\subsection{Time period for analysis}
Assign an arbitrary \texttt{start\_date} and an \texttt{end\_date}.

<<>>=
today <- Sys.Date()
start_date <- today - 184
end_date <- today
start_date
end_date
@

\subsection{S\&P 500: benchmark $\beta=1$}
The S\&P~500 is a common performance benchmark for $\beta=1$ in the stock market. 
<<>>=
fetch_fred_sp500 <- fredr_series(series_id = 'SP500',
    observation_start=start_date, observation_end=end_date)
sp500 <- data.frame(date=index(fetch_fred_sp500),
    coredata(fetch_fred_sp500))
sp500[1:3,]
@
\subsection{Fed Funds rate: benchmark $\beta=0$}
We also need a so-called ``risk-free'' rate of interest as a benchmark for $\beta=0$.
<<>>=
fetch_fred_ffr <- fredr_series(series_id = 'DFF',
    observation_start = start_date, observation_end = end_date)
ffr <- data.frame(date=index(fetch_fred_ffr),
    coredata(fetch_fred_ffr))
ffr[1:3,]
@
After fetching the data, we construct an estimated risk-free index \texttt{RFI} by making an imaginary deposit of \$1,000.00 and investing it each banking day at the reported Daily Federal Funds rate actual/365 simple interest~\footnote{Actual/365 simple interest is only a guess: as we all know, the Fed is highly secretive about its operations, and we are only \textit{estimating} for this purpose.} until the next banking day.
<<>>=
ffr$RFI[1] <- 1000.00
for(i in 2:nrow(ffr))
{
    ffr$RFI[i] <- lag(ffr$RFI)[i] *
        (1 + lag(ffr$DFF)[i] / 100 *
            (ffr$date[i]-lag(ffr$date)[i]) / 365)
}
ffr[1:3,]
@
\subsection{Stock ticker data}
<<>>=
fetch_quandl_p	<- Quandl.datatable('WIKI/PRICES',
    ticker='CDE', date.gte=start_date, date.lte=end_date,
    qopts.columns=c("date", "adj_close"))
fetch_quandl_p[1:3,]
@
We use an ``adjusted close'' price of the stock we are analyzing; namely a daily price ticker series based on actual closing price, but conveniently ``adjusted'' for dividends and share splits.
\section{Analyzing the data}
The first step in analyzing the data is to ``splice'' it together --- we have price level series for a major market index, a risk-free index, and the stock of our interest.  We will use an \mbox{\textit{inner\_join}}, because there are possibly some banking days on which the stock market is closed or vice versa.

<<>>=
j <- inner_join(
    inner_join(ffr, sp500, by="date"),
        fetch_quandl_p, by="date")
j$r0 <- log(j$RFI) - log(lag(j$RFI))
j$r1 <- log(j$SP500) - log(lag(j$SP500))
j$r2 <- log(j$adj_close) - log(lag(j$adj_close))
j$x <- j$r1 - j$r0
j$y <- j$r2 - j$r0
@

\begin{figure}
<<echo = FALSE, fig = TRUE>>=
matplot(j$date-j$date[1]+1,data.frame(j$RFI*100/j$RFI[1],
    j$SP500*100/j$SP500[1], j$adj_close*100/j$adj_close[1]),
    type="l", col=c("green", "red", "blue"), lty=1,
    xlab="days elapsed since start_date", ylab="price level starting at 100",
    main="price level performance")
legend(x="topleft", legend=c("Fed Funds", "S&P 500", "our stock"), lty=1,
    col=c("green", "red", "blue"))
@
\caption{Plot of price level data}
\end{figure}

\subsection{Fitting a linear model}
The next step is to fit a linear model to the data, 
\begin{equation}
	y_i = \alpha + \beta x_i + \epsilon_i,\qquad 1\le i\le n,
\end{equation}
where the ``Sum of Squared Error''
\begin{equation}
	SSE = \sum_{i=1}^{n}\epsilon_{i}^{2}
	= \sum_{i=1}^{n}\big(y_i-(\alpha+\beta x_i)\big)^{2}
\end{equation}
is \textit{minimized}.

This process, called \textbf{linear regression}, is very easy to do in \textsf{R}.

\subsection{Free $\alpha$ model}
This is a basic linear regression of daily logarithmic returns of the particular stock or portfolio that we are analyzing against returns of the general market.

<<>>=
model_1 <- lm(formula = y ~ x, data = j[2:nrow(j),])
summary(model_1)
@

The coefficient labeled ``\mbox{\texttt{(Intercept)}}'' is known as $\alpha$ (``alpha'') --- in this case a \textit{daily} $\alpha$ which would have to be multiplied by $252$~\footnote{Or, the number of trading days in a year.} to yield an annualized ``stock market $\alpha$'' on a logarithmic basis.

The coefficient labeled ``\mbox{\texttt{x}}'' is the estimated $\beta$ (``beta'') of the stock whose returns we are modeling against those of the market in general.  A red line is plotted in Figure~\ref{lm-plot} with the slope and intercept of this model

\subsection{Forced $\alpha=0$ model}
Now it would be rare, exceptional, and not particularly meaningful if the reported coefficient for the \mbox{\texttt{(Intercept)}} of \mbox{\texttt{model\_1}} were statistically different from zero.  The profound lack of statistical significance of estimates of $\alpha$ is strong evidence for the common saying or disclaimer: ``Past performance is no guarantee of future returns.''

Some readers may be of the philosophy that ``There Ain't No Such Thing As A Free Lunch'' \cite{henderson2014,heinlein1966}, $\alpha=0$, and therefore one free parameter may be eliminated from the model without affecting its validity.

<<>>=
model_0 <- lm(formula = y ~ x - 1, data = j[2:nrow(j),])
summary(model_0)
@

\begin{figure}
<<echo = FALSE, fig = TRUE>>=
plot(j$x, j$y, xlab="the S&P 500", ylab="our stock",
   main = "daily log returns", sub="adjusted for Fed Funds rate")
abline(reg=model_1, col="red")
abline(reg=model_0, col="green")
legend(x="bottomright", legend=c("free alpha", "forced alpha=0"),
   col=c("red","green"), lty=1)
@
\caption{Scatter plot with lines of best fit from linear model}
\label{lm-plot}
\end{figure}

\section{Interpreting the results}
The summary reports generated by \textsf{R} yield a lot of useful information on the statistical significance of the estimated coefficients. The estimates of $R^2$ are particularly significant --- $R^2$ is the fraction of the variation of the price level of a particular stock or portfolio which can be explained by the performance of the market as a whole.

For many stocks, an excessive $\beta$ together with an excessive correlation with the market in general, $R^2$, is a serious problem, and one that discorages us from investing especially in large-cap stocks.

This is not at all the problem we face with the stock we are now analyzing, (i.e., shortly after the end of the first quarter 2018.) The stock has experienced a significant drop and high general volatility in price, which are very poorly explained by our model of regression against the S\&P~500.

Therefore we need to look for other factors which may better explain day-to-day movements in the price of the stock which we are analyzing. A glance at the front page of the website \cite{coeur} of the company whose stock we are analyzing suggests that a possible next step for further research would be to fetch some commodity price data for gold and silver, and do a multiple regression.

<<>>=
m <- inner_join(data.frame(date=j$date, price=j$adj_close,
        rfi=j$RFI, sp500=j$SP500),
    inner_join(Quandl('COM/AG_USD', date.gte=start_date, date.lte=end_date),
        Quandl('COM/AU_LPM', date.gte=start_date, date.lte=end_date),
            by="date", suffix=c("_Ag", "_Au")), by="date")
# natural logarithms of relative price levels    
m$l_price <- log(m$price/m$rfi)
m$l_sp500 <- log(m$sp500/m$rfi)
m$l_ag <- log(m$value_Ag/m$rfi)
m$l_au <- log(m$value_Au/m$rfi)
# diff the logarithms from the previous day
m$r_price <- m$l_price - lag(m$l_price)
m$r_sp500 <- m$l_sp500 - lag(m$l_sp500)
m$r_ag <- m$l_ag - lag(m$l_ag)
m$r_au <- m$l_au - lag(m$l_au)
@

The data sets \mbox{\texttt{COM/AG\_USD}} and \mbox{\texttt{COM/AU\_LPM}} from \cite{quandl} are so-called ``London fix'' prices for silver and gold, respectively, in U.S.~dollars.  The term ``fix'' suggests that the prices of these commodities are excessively regulated by --- to be frank --- a criminal \textbf{cartel}, and that in reality no true free market exists for such precious metals. The fact that these ``fixes'' are reported in the tables as ``values'' rather than ``prices'' should set off further alarm bells. We have heard the ``value-vs.-price'' story from brokers\footnote{Technically, they are dealers rather than brokers: yes, you got that right, \textit{dealers} selling a daily or twice-a-day ``fix'' like they run some sort of pawn shop on the side in a really bad part of London where they buy jewelry, melt it down, and sell coins and such.} before, and we do not like what we hear.

<<>>=
model_M <- lm(data = m[2:nrow(m),], formula = r_price ~ r_sp500 + r_ag + r_au)
summary(model_M)
@

We do not like the results of this multiple regression. As we do this analysis shortly after the end of the first quarter 2018, the company's stock price shows an extremely high positive correlation with the price of gold, and a strong negative correlation with the price of silver. Such correlations\footnote{O.K., we are not using the term ``correlation'' with its precise statistical meaning $\mathrm{Corr}(x,y)=\mathrm{Covar}(x,y)/(\sigma_x\sigma_y)$. We are talking about the coefficients of the regression, i.e., the linear \textit{dependency} of the day-to-day change in price of the stock on that of the S\&P~500, silver, or gold, respectively.} sometimes expose a publicly traded mining company's hedge position in futures markets. This company may have hedged against more silver than the market believes it has in reserve, and on the other hand be inadequatedly hedged against a fall in the price of gold.  Further analysis is warranted.

This company's stock seems to be a strong bet for gold against silver. There is not really a satisfactory explanation for this.

Nor is there an explanation for the lack of accounting for platinum, palladium, and other precious metals that tend to be extracted along with gold and silver.  Nor can we explain the contemptuous zeros in the columns for ``lead'' and ``zinc'' in company's reports of ``proven'' and ``probable'' reserves or the total lack of mention of copper.

The ore may be not be of a grade that it is profitable to mine it solely for the sake of other metals than gold and silver, but at the same time, when the company has gone to the expense of digging the ore, and extracting the metals, we do not believe that other precious and valuable metals, invariably also extracted as a by-product of mining silver and gold, should simply be thrown away without accounting.

\phantomsection\addcontentsline{toc}{section}{References}\bibliography{Rab}{}

\end{document}
